{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# List of file paths to load\n",
    "file_paths = [\n",
    "    \"data/Benefits Details 2024.11.07.pdf\",\n",
    "    \"data/Benefits_Details_2024.11.05.pdf\"\n",
    "]\n",
    "\n",
    "# Load each file and store the documents in a list\n",
    "docs = []\n",
    "for file_path in file_paths:\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs.extend(loader.load())  # Append each document's contents to the `docs` list\n",
    "\n",
    "print(len(docs))  # Total number of document sections loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API and embeddings\n",
    "\n",
    "def initialize_llm_and_embeddings(api_key, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Initializes the language model and embeddings.\n",
    "    \"\"\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    llm = ChatOpenAI(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    return llm, embeddings\n",
    "\n",
    "# Example usage\n",
    "llm, embeddings = initialize_llm_and_embeddings(\"sk-proj....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Set up Chroma with local persistence\n",
    "persist_directory = \"db\"  # Set your directory here\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings model (needed for the retriever)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Reload the persisted Chroma database\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings  # Use `embedding_function` instead of `embedding`\n",
    ")\n",
    "\n",
    "# Create a retriever from the loaded vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(input, retriever, llm):\n",
    "  system_prompt = (\n",
    "      \"You are an assistant for question-answering tasks. \"\n",
    "      \"Use the following pieces of retrieved context to answer \"\n",
    "      \"the question. If you don't know the answer, say that you \"\n",
    "      \"don't know. Use three sentences maximum and keep the \"\n",
    "      \"answer concise.\"\n",
    "      \"\\n\\n\"\n",
    "      \"{context}\"\n",
    "  )\n",
    "\n",
    "  prompt = ChatPromptTemplate.from_messages(\n",
    "      [\n",
    "          (\"system\", system_prompt),\n",
    "          (\"human\", \"{input}\"),\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "  rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "  results = rag_chain.invoke({\"input\": input})\n",
    "  return results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Hilton Honors American Express Aspire Card offers 3x points on purchases like YouTube Premium, as it falls under the \"everything else\" category. The Blue Cash Preferred Card from American Express does not specifically list YouTube Premium, but it generally offers 1% cash back on such purchases. There is no differentiation between in-store and online purchases for these cards. The Hilton Honors Aspire Card is optimal for YouTube Premium due to its higher points rate.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '''\n",
    "As a knowledgeable financial advisor specializing in credit card discounts and rewards,\n",
    "your task is to analyze and compare the specific points, rewards, and benefits offered by the\n",
    "following credit cards: {Hilton Honors American Express Aspire Card, Blue Cash Preferred Card from American Express},\n",
    "for purchases at YouTube Premium. Provide clear, specific information on the points each card offers\n",
    "for purchases at this store. Specify whether the points or benefits differ for in-store versus online purchases,\n",
    "and avoid referencing unrelated benefits. Highlight which card is optimal for use.\n",
    "based on the points or cashback it offers, and provide concise comparisons where applicable.\n",
    "Limit your response to 500 characters for clarity andÂ conciseness.\n",
    "'''\n",
    "\n",
    "answer = get_answer(question, retriever, llm)\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
